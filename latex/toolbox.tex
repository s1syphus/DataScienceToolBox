\documentclass{report}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{hyperref}

\hypersetup{
	colorlinks,
	linkcolor=black
}

\pdfpagewidth 8.5in
\pdfpageheight 11in


\title{Data Science Toolbox}
\author{
	Mike Micatka
}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\renewcommand{\abstractname}{What is Data Science?}
\begin{abstract}
This is a work in progress. Mainly made for learning purposes. Hopelessly 
abbreviated, will do my best to provide sources/other resources.
\end{abstract}

\chapter{Data Techniques}

\chapter{Machine Learning Techniques}
\section{Supervised Learning}
	\subsection{Averaged One-Dependence Estimators (AODE)}
Averaged One-Dependence Estimators is a probabilistic classification technique.
It is an improvement on the \hyperref[subsec:bayesian_statistics]{Naive Bayes
 Estimator}. This technique produces class probabilities rather than single
  classes which allows more flexibility by having the end-user set the threshold
   for selection. The computational complexity for training is $O(ln^2)$ and
    $O(kn^2)$ for classification where $n$ is the number of features, $l$ is
     the number of training samples, and $k$ is the number of testing samples.
      The equation for this classifier is as follows:\\
$
\hat{P}(y|x_1, ..., x_n) = \frac{
\sum_{i:1 \leq i \leq n \wedge F(x_i) \geq m} \hat{P}(y,x_i) \Pi ^n _{j=1} \hat{P}(x_j | y, x_i)
}
{
\sum_{y'\in Y} \sum _{i:1 \leq i \leq n \wedge F(x_i) \geq m} \hat{P}(y',x_i) \Pi ^n _{j=1} \hat{P} (x_j | y', x_i)
}
$\\
Where $\hat{P}(\cdot)$ is the estimate of $P(\cdot)$, $F(\cdot)$ is the frequency of the argument in the sample data, and $m$ is the user specified minimum frequency, usually set at 1.\\
The computational complexity makes this technique infeasible for high-dimensional data but is linear with respect to the number of samples so can handle large amounts of training data. 
\\
Implementations:
\begin{itemize}
\item Weka
\end{itemize}

	\subsection{Bayesian Statistics}
	\label{subsec:bayesian_statistics}

	\subsection{Case-Based Reasoning}

	\subsection{Gaussian Process Regression}

	\subsection{Gene Expression Programming}

	\subsection{Group Method of Data Handling (GMDH)}

	\subsection{Inductive Logic Programming}

	\subsection{Instance-based Learning}

	\subsection{Lazy Learning}

	\subsection{Learning Vector Quantification}

	\subsection{Logistic Model Tree}

	\subsection{Minimum Message Length}

	\subsection{Probably Approximately Correct Learning (PAC)}

	\subsection{Ripple Down Rules}
	
	\subsection{Support Vector Machine (SVM)}
		Add in stuff about different kernels

	\subsection{Random Forests}

	\subsection{Ensemble Learning}

	\subsection{Ordinal Classification}

	\subsection{Information Fuzzy Network (IFN)}

	\subsection{Conditional Random Field}

	\subsection{ANOVA}

	\subsection{Linear Classifier}

	\subsection{Quadratic Classifier}

	\subsection{Nearest Neighbor}

	\subsection{Boosting}

	\subsection{Decision Tree}
		This will have several parts

	\subsection{Bayesian Network}

	\subsection{Hidden Markov Model}

\section{Unsupervised Learning}

	\subsection{Expectation-Maximization Algorithm}

	\subsection{Vector Quantization}

	\subsection{Generative Topographic Map}

	\subsection{Information Bottleneck Method}

	\subsection{Association Rule Learning}

		\begin{itemize}
			\item Apriori Algorithm
			\item Eclat Algorithm
			\item FP-Growth Algorithm
		\end{itemize}

	\subsection{Hierarchical Clustering}

		\begin{itemize}
			\item Single-Linkage Clustering
			\item Conceptual Clustering
		\end{itemize}

	\subsection{Cluster Analysis}

		\begin{itemize}
			\item K-Means Algorithm
			\item Fuzzy Clustering
			\item DBSCAN
			\item OPTICS Algorithm
		\end{itemize}

	\subsection{Outlier Detection}

\section{Semi-Supervised Learning}

	\subsection{Generative Model}

	\subsection{Low-Density Separation}

	\subsection{Graph-Based Methods}

	\subsection{Co-Training}

\section{Reinforcement Learning}

	\subsection{Temporal Difference Learning}

	\subsection{Q-Learning}

	\subsection{Learning Automata}

	\subsection{State-Action-Reward-State-Action (SARSA)}

\section{Deep Learning}

	\subsection{Deep Belief Network}

	\subsection{Deep Boltzmann Machine}

	\subsection{Deep Convolution Neural Networks (CNN)}

	\subsection{Deep Recurrent Neural Networks (RNN)}

	\subsection{Hierarchical Temporal Memory}
	
\chapter{Choosing the Right Algorithm}

\begin{thebibliography}

\end{thebibliography}

\end{document}

